"""
ì°¸ê³  íŒŒì¼: PDF íŒŒì‹± ë©”ì¸ ëª¨ë“ˆ

ì¶œì²˜: ê¸°ì¡´ í”„ë¡œì íŠ¸ (ì‚¬ìš©ì ì œê³µ)
ì°¸ê³  ëª©ì : PDF íŒŒì„œ êµ¬í˜„ ì°¸ê³ 

ì£¼ìš” ì°¨ì´ì  ì˜ˆìƒ:
1. ì´ëª¨ì§€ ì‚¬ìš©: ì°¸ê³  íŒŒì¼ì—ëŠ” ì´ëª¨ì§€(ğŸ”, ğŸ’¾, âœ…, âŒ ë“±)ê°€ ìˆìœ¼ë‚˜, í˜„ì¬ í”„ë¡œì íŠ¸ëŠ” ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€ â†’ `[INFO]`, `[ERROR]` í˜•ì‹ìœ¼ë¡œ ë³€ê²½ í•„ìš”
2. ë¡œê¹… í˜•ì‹: ì°¸ê³  íŒŒì¼ì€ ì¼ë°˜ ë¡œê¹…, í˜„ì¬ í”„ë¡œì íŠ¸ëŠ” `[INFO]`, `[ERROR]` í˜•ì‹ ì‚¬ìš©
3. ì–‘ë©´ ë¶„ë¦¬ ë¡œì§: ì°¸ê³  íŒŒì¼ì—ëŠ” `_split_pages_by_side()` ë©”ì„œë“œê°€ ìˆìœ¼ë‚˜, í˜„ì¬ í”„ë¡œì íŠ¸ì—ëŠ” í•„ìš” ì—¬ë¶€ í™•ì¸ í•„ìš”
4. clean_output ì˜µì…˜: ì°¸ê³  íŒŒì¼ì—ëŠ” `clean_output` ì˜µì…˜ì´ ìˆìœ¼ë‚˜, í˜„ì¬ í”„ë¡œì íŠ¸ì—ëŠ” ì—†ì„ ìˆ˜ ìˆìŒ
5. ì„¤ì • ê´€ë¦¬: ì°¸ê³  íŒŒì¼ì€ `settings.upstage_api_key` ì§ì ‘ ì‚¬ìš©, í˜„ì¬ í”„ë¡œì íŠ¸ëŠ” `Settings` í´ë˜ìŠ¤ ì‚¬ìš©
6. ë³€ìˆ˜ëª…/í•¨ìˆ˜ëª…: í˜„ì¬ í”„ë¡œì íŠ¸ ê·œì¹™ì— ë§ê²Œ Align í•„ìš”
7. ìºì‹± í†µí•©: ì°¸ê³  íŒŒì¼ì˜ ìºì‹± ë¡œì§ì„ í˜„ì¬ í”„ë¡œì íŠ¸ì˜ `CacheManager`ì™€ í†µí•© í•„ìš”

Upstage APIë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ PDFë¥¼ íŒŒì‹±í•˜ê³ ,
ì–‘ë©´ ìŠ¤ìº” ë¶„ë¦¬ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import os
import logging
from typing import Dict, Any, List
from pathlib import Path
from bs4 import BeautifulSoup
import re

from backend.config.settings import settings
from backend.parsers.upstage_api_client import UpstageAPIClient
from backend.parsers.cache_manager import CacheManager

logger = logging.getLogger(__name__)


class PDFParser:
    """
    PDF íŒŒì‹± ë©”ì¸ í´ë˜ìŠ¤

    Flow:
    1. ìºì‹œ í™•ì¸
    2. ìºì‹œ ë¯¸ìŠ¤ ì‹œ Upstage API í˜¸ì¶œ ë° ìºì‹±
    3. API ì‘ë‹µ â†’ êµ¬ì¡°í™”ëœ Elements ë³€í™˜
    4. ì–‘ë©´ ë¶„ë¦¬ ë¡œì§ ì ìš©
    5. ìµœì¢… JSON ë°˜í™˜
    """

    def __init__(self, enable_cache: bool = True, clean_output: bool = True):
        """
        Args:
            enable_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            clean_output: ì¶œë ¥ ì‹œ ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±° (original_page, page)
        """
        self.upstage_client = UpstageAPIClient(settings.upstage_api_key)
        self.cache_manager = CacheManager() if enable_cache else None
        self.clean_output = clean_output

    def parse_pdf(
        self, pdf_path: str, use_cache: bool = True, force_split: bool = False
    ) -> Dict[str, Any]:
        """
        PDF íŒŒì‹± ë©”ì¸ í•¨ìˆ˜

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            force_split: ê°•ì œ ì–‘ë©´ ë¶„ë¦¬ ì—¬ë¶€

        Returns:
            {
                "success": True,
                "pages": [
                    {
                        "page_number": 1,
                        "original_page": 1,
                        "side": "left",
                        "elements": [
                            {
                                "id": 0,
                                "page": 1,
                                "text": "...",
                                "category": "paragraph",
                                "font_size": 20,
                                "bbox": {"x0": 0.1, "y0": 0.2, ...}
                            },
                            ...
                        ]
                    },
                    ...
                ],
                "total_pages": 4,
                "original_pages": 2,
                "split_applied": True,
                "metadata": {...}
            }
        """
        try:
            pdf_path = Path(pdf_path)
            if not pdf_path.exists():
                raise FileNotFoundError(f"PDF not found: {pdf_path}")

            logger.info(f"ğŸ” Parsing PDF: {pdf_path.name}")

            # 1. ìºì‹œ í™•ì¸
            api_response = None
            if use_cache and self.cache_manager:
                api_response = self.cache_manager.get_cached_result(str(pdf_path))
                if api_response:
                    logger.info(f"ğŸ’¾ Using cached API response for {pdf_path.name}")

            # 2. API í˜¸ì¶œ
            if api_response is None:
                api_response = self.upstage_client.parse_pdf(str(pdf_path))

                # ìºì‹± (API ì‘ë‹µ ì›ë³¸ ê·¸ëŒ€ë¡œ)
                if use_cache and self.cache_manager:
                    self.cache_manager.save_cache(str(pdf_path), api_response)
                    logger.info(f"ğŸ’¾ Cached API response for {pdf_path.name}")

            # 3. Elements êµ¬ì¡°í™”
            logger.info("ğŸ”§ Structuring elements...")
            structured_elements = self._structure_elements(api_response)

            # 4. ì–‘ë©´ ë¶„ë¦¬
            logger.info("ğŸ“„ Splitting pages by side...")
            pages = self._split_pages_by_side(structured_elements, force_split)

            # 5. clean_output ì²˜ë¦¬ (ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°)
            if self.clean_output:
                pages = self._clean_pages(pages)
            
            # 6. ìµœì¢… ê²°ê³¼
            original_pages = api_response.get("usage", {}).get("pages", 0)
            result = {
                "success": True,
                "pages": pages,
                "total_pages": len(pages),
                "original_pages": original_pages,
                "split_applied": len(pages) > original_pages,
                "force_split_applied": force_split,
                "pdf_path": str(pdf_path),
                "metadata": {
                    "api_version": api_response.get("api"),
                    "model": api_response.get("model"),
                    "processing_applied": {
                        "upstage_parsing": True,
                        "element_structuring": True,
                        "page_splitting": len(pages) > original_pages,
                    },
                },
            }

            logger.info(
                f"âœ… Parsing completed: {original_pages} original pages â†’ {len(pages)} final pages"
            )
            return result

        except Exception as e:
            logger.error(f"âŒ PDF parsing failed: {e}")
            raise

    def _structure_elements(self, api_response: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        API ì‘ë‹µì˜ elementsë¥¼ ìš°ë¦¬ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”

        Input: api_response["elements"] = [
            {
                "id": 0,
                "page": 1,
                "category": "paragraph",
                "coordinates": [{"x": 0.1, "y": 0.2}, ...],
                "content": {"html": "<p>...</p>", ...}
            },
            ...
        ]

        Output: [
            {
                "id": 0,
                "page": 1,
                "text": "í…ìŠ¤íŠ¸ ë‚´ìš©",
                "category": "paragraph",
                "font_size": 20,
                "bbox": {"x0": 0.1, "y0": 0.2, "x1": 0.5, "y1": 0.3, "width": 0.4, "height": 0.1}
            },
            ...
        ]
        """
        elements = api_response.get("elements", [])
        structured = []

        for elem in elements:
            # HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            html_content = elem.get("content", {}).get("html", "")
            text = self._extract_text_from_html(html_content)

            # Font size ì¶”ì¶œ
            font_size = self._extract_font_size(html_content)

            # Bbox ê³„ì‚°
            bbox = self._calculate_bbox(elem.get("coordinates", []))

            structured.append(
                {
                    "id": elem.get("id"),
                    "page": elem.get("page"),  # ë‚´ë¶€ ì²˜ë¦¬ìš© (ì–‘ë©´ ë¶„ë¦¬ì— í•„ìš”)
                    "text": text,
                    "category": elem.get("category", "unknown"),
                    "font_size": font_size,
                    "bbox": bbox,
                }
            )

        logger.info(f"Structured {len(structured)} elements")
        return structured

    def _extract_text_from_html(self, html: str) -> str:
        """HTMLì—ì„œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not html:
            return ""
        soup = BeautifulSoup(html, "html.parser")
        return soup.get_text(strip=True)

    def _extract_font_size(self, html: str) -> int:
        """HTML styleì—ì„œ font-size ì¶”ì¶œ"""
        if not html:
            return 12
        match = re.search(r"font-size:(\d+)px", html)
        return int(match.group(1)) if match else 12

    def _calculate_bbox(self, coordinates: List[Dict]) -> Dict[str, float]:
        """ì¢Œí‘œ ë°°ì—´ì—ì„œ bbox ê³„ì‚°"""
        if not coordinates:
            return {"x0": 0, "y0": 0, "x1": 0, "y1": 0, "width": 0, "height": 0}

        x_coords = [c["x"] for c in coordinates]
        y_coords = [c["y"] for c in coordinates]

        x0, x1 = min(x_coords), max(x_coords)
        y0, y1 = min(y_coords), max(y_coords)

        return {
            "x0": x0,
            "y0": y0,
            "x1": x1,
            "y1": y1,
            "width": x1 - x0,
            "height": y1 - y0,
        }

    def _split_pages_by_side(
        self, elements: List[Dict[str, Any]], force_split: bool
    ) -> List[Dict[str, Any]]:
        """
        í˜ì´ì§€ë³„ ì–‘ë©´ ë¶„ë¦¬ (ìƒëŒ€ì¢Œí‘œ ê¸°ì¤€ 0.5 ê³ ì •)

        ì¢Œí‘œê°€ ì •ê·œí™”ëœ ìƒëŒ€ì¢Œí‘œì´ë¯€ë¡œ:
        - x < 0.5: ì¢Œì¸¡ í˜ì´ì§€
        - x >= 0.5: ìš°ì¸¡ í˜ì´ì§€
        """
        CENTERLINE = 0.5  # ê³ ì • ì¤‘ì•™ì„ 

        # í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”
        pages_dict = {}
        for elem in elements:
            page_num = elem["page"]
            if page_num not in pages_dict:
                pages_dict[page_num] = []
            pages_dict[page_num].append(elem)

        # í˜ì´ì§€ë³„ë¡œ ì¢Œ/ìš° ë¶„ë¦¬
        result_pages = []
        page_counter = 1

        for original_page in sorted(pages_dict.keys()):
            page_elements = pages_dict[original_page]

            # ì¢Œ/ìš° ë¶„ë¦¬ (ê³ ì • ì¤‘ì•™ì„  0.5 ê¸°ì¤€)
            left_elements = [e for e in page_elements if e["bbox"]["x0"] < CENTERLINE]
            right_elements = [e for e in page_elements if e["bbox"]["x0"] >= CENTERLINE]

            logger.debug(
                f"  Page {original_page}: {len(page_elements)} elements â†’ "
                f"{len(left_elements)} left, {len(right_elements)} right "
                f"(centerline={CENTERLINE})"
            )

            # ì¢Œì¸¡ í˜ì´ì§€ (ìš”ì†Œê°€ ìˆì„ ê²½ìš°ë§Œ)
            if left_elements:
                result_pages.append(
                    {
                        "page_number": page_counter,
                        "original_page": original_page,
                        "side": "left",
                        "elements": sorted(
                            left_elements,
                            key=lambda x: (x["bbox"]["y0"], x["bbox"]["x0"]),
                        ),
                        "metadata": {
                            "is_split": True,
                            "centerline": CENTERLINE,
                            "element_count": len(left_elements),
                        },
                    }
                )
                page_counter += 1

            # ìš°ì¸¡ í˜ì´ì§€ (ìš”ì†Œê°€ ìˆì„ ê²½ìš°ë§Œ)
            if right_elements:
                result_pages.append(
                    {
                        "page_number": page_counter,
                        "original_page": original_page,
                        "side": "right",
                        "elements": sorted(
                            right_elements,
                            key=lambda x: (x["bbox"]["y0"], x["bbox"]["x0"]),
                        ),
                        "metadata": {
                            "is_split": True,
                            "centerline": CENTERLINE,
                            "element_count": len(right_elements),
                        },
                    }
                )
                page_counter += 1

        logger.info(f"Page splitting completed: {len(pages_dict)} original pages â†’ {len(result_pages)} split pages")
        return result_pages

    def _clean_pages(self, pages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±° (clean_output=Trueì¼ ë•Œ)
        
        ì œê±°í•  í•„ë“œ:
        - í˜ì´ì§€ ë ˆë²¨: original_page
        - element ë ˆë²¨: page
        """
        import copy
        cleaned_pages = copy.deepcopy(pages)
        
        for page in cleaned_pages:
            # original_page ì œê±°
            if "original_page" in page:
                del page["original_page"]
            
            # elements ë‚´ì˜ page í•„ë“œ ì œê±°
            if "elements" in page:
                for element in page["elements"]:
                    if "page" in element:
                        del element["page"]
        
        return cleaned_pages
